name: Fetch full sitemap (Python)

on:
  schedule:
    - cron: '0 0 * * *'   # —Ä–∞–∑ –≤ —á–∞—Å
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: python -m pip install --upgrade pip && python -m pip install requests

      - name: Download & expand sitemap into sitemap_full.txt
        env:
          MAIN_SITEMAP: "https://salomon.community/sitemap.xml"  # <- –µ—Å–ª–∏ –ø–æ–º–µ–Ω—è–µ—Ç—Å—è, –∑–∞–º–µ–Ω–∏ –∑–¥–µ—Å—å
          OUTPUT_FILE: "sitemap_full.txt"
        run: |
          python - <<'PY'
          import requests, sys, xml.etree.ElementTree as ET, os, re

          MAIN = os.environ.get("MAIN_SITEMAP")
          OUT = os.environ.get("OUTPUT_FILE", "sitemap_full.txt")
          headers = {"User-Agent": "CacheWarmerBot/1.1 (+https://salomon.community)"}

          def fetch(url):
              try:
                  r = requests.get(url, headers=headers, timeout=30)
                  r.raise_for_status()
                  return r.content
              except Exception as e:
                  print("ERROR fetching", url, e)
                  return None

          def extract_locs(bts):
              """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ <loc> –∏–∑ XML (—É—á–∏—Ç—ã–≤–∞—è namespace)."""
              urls = []
              try:
                  root = ET.fromstring(bts)
                  for el in root.findall('.//{*}loc'):
                      if el.text:
                          urls.append(el.text.strip())
              except Exception as e:
                  print("XML parse error:", e)
              return urls

          def is_valid_page(url):
              """–û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏—Å–∫–ª—é—á–∞—è –º–µ–¥–∏–∞ –∏ —Ñ–∏–¥—ã."""
              url_lower = url.lower()
              # –ò—Å–∫–ª—é—á–∞–µ–º –º–µ–¥–∏–∞-—Ñ–∞–π–ª—ã –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ URL
              if re.search(r'\.(jpg|jpeg|png|gif|webp|svg|avif|pdf|mp4|webm|xml|xsl)$', url_lower):
                  return False
              if any(x in url_lower for x in ['feed', 'wp-json', '?', '#']):
                  return False
              return True

          print("üõ∞ Fetching main sitemap:", MAIN)
          content = fetch(MAIN)
          if not content:
              print("Failed to fetch main sitemap.")
              sys.exit(1)

          try:
              root = ET.fromstring(content)
              nested = [el.text.strip() for el in root.findall('.//{*}sitemap/{*}loc') if el.text]
          except Exception as e:
              print("Parse error:", e)
              nested = []

          all_urls = []

          if nested:
              print(f"üîç Found {len(nested)} nested sitemap(s).")
              for sm in nested:
                  print("  ‚Üí", sm)
                  b = fetch(sm)
                  if not b:
                      continue
                  locs = extract_locs(b)
                  all_urls.extend(locs)
          else:
              locs = extract_locs(content)
              all_urls.extend(locs)

          # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
          filtered = [u for u in all_urls if is_valid_page(u)]
          filtered = sorted(set(filtered))

          print(f"‚úÖ Total pages after filtering: {len(filtered)}")

          with open(OUT, "w", encoding="utf-8") as f:
              for u in filtered:
                  f.write(u + "\n")

          print("üìù Saved to", OUT)
          PY

      - name: Commit & push sitemap_full.txt
        run: |
          git config user.name "Sitemap Updater"
          git config user.email "actions@github.com"
          git add sitemap_full.txt || true
          git commit -m "update full sitemap $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes to commit"
          git push
