name: Warm cache from sitemap (Python)

on:
  schedule:
    - cron: '0 * * * *'   # –∑–∞–ø—É—Å–∫ —Ä–∞–∑ –≤ —á–∞—Å
  workflow_dispatch:

jobs:
  warm:
    runs-on: ubuntu-latest
    steps:
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: python -m pip install --upgrade pip && python -m pip install requests

      - name: Warm cache from sitemap
        env:
          MAIN_SITEMAP: "https://salomon.community/sitemap.xml"  # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞–º–µ–Ω–∏
          DELAY: "2"  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫.)
        run: |
          python - <<'PY'
          import requests, time, xml.etree.ElementTree as ET, re, os

          MAIN = os.environ.get("MAIN_SITEMAP")
          DELAY = float(os.environ.get("DELAY", "2"))
          headers = {"User-Agent": "CacheWarmerBot/2.0 (+https://salomon.community)"}

          def fetch(url):
              try:
                  r = requests.get(url, headers=headers, timeout=30)
                  r.raise_for_status()
                  return r.content
              except Exception as e:
                  print("‚ùå Error fetching:", url, e)
                  return None

          def extract_locs(bts):
              urls = []
              try:
                  root = ET.fromstring(bts)
                  for el in root.findall('.//{*}loc'):
                      if el.text:
                          urls.append(el.text.strip())
              except Exception as e:
                  print("XML parse error:", e)
              return urls

          def is_valid_page(url):
              url_lower = url.lower()
              if re.search(r'\.(jpg|jpeg|png|gif|webp|svg|avif|pdf|mp4|webm|xml|xsl)$', url_lower):
                  return False
              if any(x in url_lower for x in ['feed', 'wp-json', '?', '#']):
                  return False
              return True

          print("üõ∞ Fetching main sitemap:", MAIN)
          content = fetch(MAIN)
          if not content:
              print("Failed to fetch main sitemap.")
              raise SystemExit(1)

          # –Ω–∞—Ö–æ–¥–∏–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–∞–π—Ç–º–∞–ø—ã
          try:
              root = ET.fromstring(content)
              nested = [el.text.strip() for el in root.findall('.//{*}sitemap/{*}loc') if el.text]
          except Exception as e:
              print("Parse error:", e)
              nested = []

          all_urls = []

          if nested:
              print(f"üîç Found {len(nested)} nested sitemap(s).")
              for sm in nested:
                  print("  ‚Üí", sm)
                  b = fetch(sm)
                  if not b:
                      continue
                  locs = extract_locs(b)
                  all_urls.extend(locs)
          else:
              locs = extract_locs(content)
              all_urls.extend(locs)

          filtered = [u for u in all_urls if is_valid_page(u)]
          filtered = sorted(set(filtered))
          print(f"‚úÖ Total pages to warm: {len(filtered)}")

          warmed = 0
          for url in filtered:
              try:
                  r = requests.get(url, headers=headers, timeout=15)
                  print(f"üî• {r.status_code} {url}")
                  warmed += 1
                  time.sleep(DELAY)
              except Exception as e:
                  print(f"‚ö†Ô∏è Failed {url}: {e}")

          print(f"üèÅ Done! Warmed {warmed}/{len(filtered)} pages.")
          PY
